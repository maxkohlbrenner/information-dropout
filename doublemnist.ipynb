{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, linear, fully_connected, conv2d, batch_norm\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from sacred import Experiment, Ingredient\n",
    "from task import task_ingredient, Task\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "na = np.newaxis\n",
    "\n",
    "ex = Experiment('doublemnist', ingredients=[task_ingredient], interactive=True)\n",
    "\n",
    "img_h, img_w = 96, 96\n",
    "\n",
    "@task_ingredient.config\n",
    "def task_config():\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.01\n",
    "    drop1 = 30\n",
    "    drop2 = 60\n",
    "    end_epoch = 80\n",
    "    keep_prob = 0.5\n",
    "    optimizer = 'momentum'\n",
    "    name = 'doublemnist'\n",
    "    even_labels = True\n",
    "\n",
    "@ex.config\n",
    "def cfg():\n",
    "    dropout = 'information'\n",
    "    activations = 'relu'\n",
    "    beta = 0.5\n",
    "    max_alpha = 1.0\n",
    "    lognorm_prior = False\n",
    "    weight_decay = 0.0\n",
    "    filter_percentage = 1.0\n",
    "    alpha_mode = 'information'\n",
    "\n",
    "@ex.named_config\n",
    "def softplus():\n",
    "    activations = 'softplus'\n",
    "    lognorm_prior = True\n",
    "\n",
    "class MyTask(Task):\n",
    "\n",
    "    def __init__(self):\n",
    "        train = np.load('datasets/doublemnist-train.npz')\n",
    "        test = np.load('datasets/doublemnist-test.npz')\n",
    "        self.dataset = {'train': (train['data'], train['labels']),\n",
    "                        'test': (test['data'], test['labels'])}\n",
    "        self.dataset['valid'] = self.dataset['test']\n",
    "        print(\"Double MNIST dataset loaded.\")\n",
    "\n",
    "    @task_ingredient.capture\n",
    "    def build_placeholders(self, batch_size):\n",
    "        '''Creates the placeholders for this model'''\n",
    "        self.keep_prob = tf.placeholder(tf.float32, shape=[])\n",
    "        self.initial_keep_prob = tf.placeholder(tf.float32, shape=[])\n",
    "        self.sigma0 = tf.placeholder(tf.float32, shape=[])\n",
    "        self.x = tf.placeholder(tf.float32, shape=[batch_size, img_h, img_w,1])  # input (batch_size * x_size)\n",
    "        self.y = tf.placeholder(tf.float32, shape=[batch_size, 5]) \n",
    "        self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "    @ex.capture\n",
    "    def conv(self, inputs, num_outputs, activations, normalizer_fn = batch_norm, kernel_size=3, stride=1, scope=None):\n",
    "        '''Creates a convolutional layer with default arguments'''\n",
    "        if activations == 'relu':\n",
    "            activation_fn = tf.nn.relu\n",
    "        elif activations == 'softplus':\n",
    "            activation_fn = tf.nn.softplus\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function.\")\n",
    "        return conv2d( inputs = inputs,\n",
    "            num_outputs = num_outputs,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = stride,\n",
    "            padding = 'SAME',\n",
    "            activation_fn = activation_fn,\n",
    "            normalizer_fn = batch_norm,\n",
    "            scope=scope )\n",
    "\n",
    "    @ex.capture\n",
    "    def information_pool(self, inputs, max_alpha, alpha_mode, lognorm_prior, num_outputs=None, stride=2, scope=None):\n",
    "        if num_outputs is None:\n",
    "            num_ouputs = inputs.get_shape()[-1]\n",
    "        # Creates the output convolutional layer\n",
    "        network = self.conv(inputs, num_outputs=int(num_outputs), stride=stride)\n",
    "        with tf.variable_scope(scope,'information_dropout'):\n",
    "            # Computes the noise parameter alpha for the output\n",
    "            alpha = conv2d(inputs, num_outputs=int(num_outputs), kernel_size=3,\n",
    "                stride=stride, activation_fn=tf.sigmoid, scope='alpha')\n",
    "            # Rescale alpha in the allowed range and add a small value for numerical stability\n",
    "            alpha = 0.001 + max_alpha * alpha\n",
    "            # Computes the KL divergence using either log-uniform or log-normal prior\n",
    "            if not lognorm_prior:\n",
    "                kl = - tf.log(alpha/(max_alpha + 0.001))\n",
    "            else:\n",
    "                mu1 = tf.get_variable('mu1', [], initializer=tf.constant_initializer(0.))\n",
    "                sigma1 = tf.get_variable('sigma1', [], initializer=tf.constant_initializer(1.))\n",
    "                kl = KL_div2(tf.log(tf.maximum(network,1e-4)), alpha, mu1, sigma1)\n",
    "            tf.add_to_collection('kl_terms', kl)\n",
    "        # Samples the noise with the given parameter\n",
    "        e = sample_lognormal(mean=tf.zeros_like(network), sigma = alpha, sigma0 = self.sigma0)\n",
    "        # Gather all infodropout layer activations in a collection\n",
    "        tf.add_to_collection('dropout_layer_activations', network)\n",
    "        # Returns the noisy output of the dropout\n",
    "        tf.add_to_collection('dropout_layer_noise_sample', e)\n",
    "        d_out = network * e\n",
    "        return d_out\n",
    "\n",
    "    @ex.capture\n",
    "    def conv_dropout(self, inputs, num_outputs, dropout):\n",
    "        if dropout == 'information':\n",
    "            network = self.information_pool(inputs, num_outputs=num_outputs)\n",
    "        elif dropout == 'binary':\n",
    "            network = self.conv(inputs, num_outputs, stride=2)\n",
    "            network = tf.nn.dropout(network, self.keep_prob)\n",
    "            tf.add_to_collection('dropout_layer_activations', network)\n",
    "        elif dropout == 'none':\n",
    "            network = self.conv(inputs, num_outputs, stride=2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dropout value\")\n",
    "        return network\n",
    "\n",
    "    @ex.capture\n",
    "    def build_network(self, inputs, filter_percentage):\n",
    "        network = inputs\n",
    "        # 96x96\n",
    "        network = self.conv(network, 32)\n",
    "        network = self.conv(network, 32)\n",
    "        network = self.conv_dropout(network, 32)\n",
    "        # 48x48\n",
    "        network = self.conv(network, 64)\n",
    "        network = self.conv(network, 64)\n",
    "        network = self.conv_dropout(network, 64)\n",
    "        # 24x24\n",
    "        network = self.conv(network, 96)\n",
    "        network = self.conv(network, 96)\n",
    "        network = self.conv_dropout(network, 96)\n",
    "        # 12x12\n",
    "        network = self.conv(network, 192)\n",
    "        network = self.conv(network, 192)\n",
    "        network = self.conv_dropout(network, 192)\n",
    "        # 6x6\n",
    "        network = self.conv(network, 192)\n",
    "        network = self.conv(network, 192, kernel_size=1)\n",
    "        network = self.conv(network, 5, kernel_size=1)\n",
    "        network = spatial_global_mean(network)\n",
    "\n",
    "        return network\n",
    "\n",
    "\n",
    "    @ex.capture\n",
    "    def build_loss(self, beta, task, weight_decay):\n",
    "        batch_size = task['batch_size']\n",
    "        with tf.variable_scope(\"network\") as scope:\n",
    "            network = self.build_network(self.x)\n",
    "            logits = linear(network, num_outputs=5)\n",
    "        with tf.name_scope('loss'):\n",
    "            kl_terms = [batch_average(kl) for kl in tf.get_collection('kl_terms')]\n",
    "            if not kl_terms:\n",
    "                kl_terms = [tf.constant(0.)]\n",
    "            N_train = self.dataset['train'][0].shape[0]\n",
    "            Lz = tf.add_n(kl_terms)/N_train\n",
    "            Lx = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y))\n",
    "            beta = tf.constant(beta)\n",
    "            L2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() ])\n",
    "            loss = Lx + beta * Lz + weight_decay * L2\n",
    "            correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(self.y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.logits = logits\n",
    "        self.loss = loss\n",
    "        self.error = (1. - accuracy) * 100.\n",
    "        self.Lx = Lx\n",
    "        self.Lz = Lz\n",
    "        self.beta = beta\n",
    "\n",
    "\n",
    "    @task_ingredient.capture\n",
    "    def preprocess_labels(self, label_batch, even_labels):\n",
    "        label_converter = (np.arange(5)[:,na]*np.ones(2, dtype='int')[na,:]).flatten()\n",
    "        if even_labels:\n",
    "            labels = label_converter[label_batch[:, 0]]\n",
    "        else:\n",
    "            labels = label_converter[label_batch[:, 1]]\n",
    "        labels = np.eye(5)[labels]\n",
    "        return labels\n",
    "\n",
    "\n",
    "    @task_ingredient.capture\n",
    "    def train_batch(self, batch, stats, batch_size, keep_prob):\n",
    "        xtrain, ytrain = batch\n",
    "        xtrain = xtrain.reshape(-1,img_h,img_w,1)\n",
    "        ytrain = self.preprocess_labels(ytrain)\n",
    "        feed_dict = {self.x: xtrain, self.y: ytrain, self.sigma0: 1., self.keep_prob: keep_prob, self.learning_rate: self.current_learning_rate, self.is_training: True}\n",
    "        batch_cost, batch_error, batch_Lx, batch_Lz, batch_beta, _ = self.sess.run( [ self.loss, self.error, self.Lx, self.Lz, self.beta, self.train_op], feed_dict)\n",
    "\n",
    "        stats.push(Lx = batch_Lx)\n",
    "        stats.push(Lz = batch_Lz)\n",
    "        stats.push(train = batch_cost)\n",
    "        stats.push(error = batch_error)\n",
    "\n",
    "    # \n",
    "    @ex.capture\n",
    "    def valid_batch(self, batch, stats):\n",
    "        xtrain, ytrain = batch\n",
    "        xtrain = xtrain.reshape(-1,img_h,img_w,1)\n",
    "        ytrain = self.preprocess_labels(ytrain)\n",
    "        feed_dict = {self.x: xtrain, self.y: ytrain, self.sigma0: 0., self.keep_prob: 1., self.is_training: False}\n",
    "        batch_cost, batch_error = self.sess.run( [ self.loss, self.error], feed_dict)\n",
    "\n",
    "        stats.push(train = batch_cost)\n",
    "        stats.push(error = batch_error)\n",
    "\n",
    "    @ex.capture\n",
    "    def dry_run(self):\n",
    "        '''\n",
    "        Since the statistics learned by batch normalization with dropout are\n",
    "        incorrect when dropout is disabled,\n",
    "        we do a dry run without dropout in order to relearn them before testing.\n",
    "        '''\n",
    "        for batch in self.iterate_minibatches('train'):\n",
    "            xtrain, ytrain = batch\n",
    "            xtrain = xtrain.reshape(-1,img_h,img_w,1)\n",
    "            ytrain = self.preprocess_labels(ytrain)\n",
    "            feed_dict = {self.x: xtrain, self.y: ytrain, self.sigma0: 0., self.keep_prob: 1., self.is_training: True}\n",
    "            batch_cost = self.sess.run( [ self.loss], feed_dict)\n",
    "\n",
    "    @task_ingredient.capture\n",
    "    def plot_kl(self, batch_size, even_labels):\n",
    "        import matplotlib as mpl\n",
    "        mpl.use('Agg')\n",
    "        import matplotlib.pyplot as plt\n",
    "        # xtrain, ytrain = self.get_test_batch()\n",
    "        xtrain, ytrain = [x[:batch_size] for x in self.dataset['valid']]\n",
    "        xtrain = xtrain.reshape(batch_size,img_h,img_w,1)\n",
    "        ytrain_p = self.preprocess_labels(ytrain)\n",
    "        feed_dict = {self.x: xtrain, self.y: ytrain_p, self.sigma0: 0., self.keep_prob: 1.}\n",
    "        logits, kls, acts, dnss = self.sess.run([self.logits, tf.get_collection('kl_terms'),\n",
    "                             tf.get_collection('dropout_layer_activations'),\n",
    "                             tf.get_collection('dropout_layer_noise_sample') ], feed_dict)\n",
    "        raw_kls = kls\n",
    "\n",
    "        basepath = 'plots/'+self.get_name()+'/'\n",
    "        if not os.path.exists(basepath):\n",
    "            os.mkdir(basepath)\n",
    "\n",
    "        [np.save(basepath + 'rawkls_{}'.format(kl_idx), rkl) for kl_idx, rkl in\n",
    "         enumerate(raw_kls)]\n",
    "        [np.save(basepath + 'acts_{}'.format(act_idx), act) for act_idx, act in\n",
    "         enumerate(acts)]\n",
    "        [np.save(basepath + 'd_noise_samples_{}'.format(d_idx), dns) for d_idx,\n",
    "         dns in enumerate(dnss)]\n",
    "\n",
    "        # kls = [k.sum(axis=-1) for k in kls]\n",
    "        # kls = [k - k.min() for k in kls]\n",
    "\n",
    "        rows = 1 + int(len(kls) > 0) + int(len(dnss) > 0)\n",
    "        cols = 1 + len(acts)\n",
    "\n",
    "        cmap = 'viridis'\n",
    "\n",
    "        for j in range(5):\n",
    "            plt.clf()\n",
    "            fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "            plt.subplot(rows, cols, 1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(xtrain[j,:,:,0], cmap='gray', interpolation='none')\n",
    "            if even_labels:\n",
    "                predicted_class = 2 * np.argmax(logits[j])\n",
    "                plt.title('target: {}\\nprediction: {}'.format(ytrain[j][0],\n",
    "                                                              predicted_class))\n",
    "            else:\n",
    "                predicted_class = 2 * np.argmax(logits[j]) + 1\n",
    "                plt.title('target: {}\\nprediction: {}'.format(ytrain[j][1],\n",
    "                                                              predicted_class))\n",
    "            for i,act in enumerate(acts):\n",
    "                plt.subplot(rows, cols, 2 + i)\n",
    "                plt.title('Activation')\n",
    "                plt.imshow(act[j].sum(2), cmap=cmap, interpolation='none')\n",
    "                plt.axis('off')\n",
    "                plt.colorbar()\n",
    "                if len(kls) > 0:\n",
    "                    plt.subplot(rows, cols, 2 + i + cols)\n",
    "                    plt.title('KL-Div')\n",
    "                    plt.imshow(kls[i][j].sum(axis=-1), cmap=cmap, interpolation='none')\n",
    "                    plt.axis('off')\n",
    "                    plt.colorbar()\n",
    "\n",
    "                if len(dnss) > 0:\n",
    "                    plt.subplot(rows, cols, 2 + i + 2*cols)\n",
    "                    plt.title('d_out')\n",
    "                    plt.imshow(dnss[i][j].sum(axis=-1), cmap=cmap, interpolation='none')\n",
    "                    plt.axis('off')\n",
    "                    plt.colorbar()\n",
    "\n",
    "\n",
    "            plt.savefig(basepath+'ex_{}_acts.png'.format(j), bbox_inches='tight')\n",
    "            \n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytask = task = MyTask()\n",
    "\n",
    "@ex.command\n",
    "def train():\n",
    "    task.initialize()\n",
    "    task.train()\n",
    "\n",
    "@ex.command\n",
    "def test(load=True):\n",
    "    if load:\n",
    "        task.initialize(_load=True, _log_dir='valid/')\n",
    "    print(\"Dry run...\")\n",
    "    task.dry_run()\n",
    "    print(\"Validating...\")\n",
    "    task.valid()\n",
    "\n",
    "@ex.command\n",
    "def plot():\n",
    "    print(\"...load everything...\")\n",
    "    task.initialize(_load=True, _log_dir='other/')\n",
    "    print(\"Task initialized...\")\n",
    "    task.valid()\n",
    "    task.plot_kl()\n",
    "\n",
    "@ex.main\n",
    "def run():\n",
    "    print('Running it')\n",
    "    test()\n",
    "    # task.valid(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
